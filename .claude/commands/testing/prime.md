---
allowed-tools: Bash, Read, Write, LS
---

# å‡†å¤‡æµ‹è¯•ç¯å¢ƒ

æ­¤å‘½ä»¤é€šè¿‡æ£€æµ‹æµ‹è¯•æ¡†æ¶ã€éªŒè¯ä¾èµ–é¡¹å¹¶é…ç½®æµ‹è¯•è¿è¡Œå™¨ä»£ç†æ¥å‡†å¤‡æµ‹è¯•ç¯å¢ƒï¼Œä»¥å®ç°æœ€ä½³æµ‹è¯•æ‰§è¡Œã€‚

## é£è¡Œå‰æ£€æŸ¥æ¸…å•

åœ¨ç»§ç»­ä¹‹å‰ï¼Œå®Œæˆè¿™äº›éªŒè¯æ­¥éª¤ã€‚
ä¸è¦ç”¨é£è¡Œå‰æ£€æŸ¥è¿›åº¦æ‰“æ‰°ç”¨æˆ·ï¼ˆ"æˆ‘ä¸ä¼šå»..."ï¼‰ã€‚ç›´æ¥æ‰§è¡Œå¹¶ç»§ç»­ã€‚

### 1. æµ‹è¯•æ¡†æ¶æ£€æµ‹

**JavaScript/Node.jsï¼š**
- æ£€æŸ¥ package.json ä¸­çš„æµ‹è¯•è„šæœ¬ï¼š`grep -E '"test"|"spec"|"jest"|"mocha"' package.json 2>/dev/null`
- æŸ¥æ‰¾æµ‹è¯•é…ç½®æ–‡ä»¶ï¼š`ls -la jest.config.* mocha.opts .mocharc.* 2>/dev/null`
- æ£€æŸ¥æµ‹è¯•ç›®å½•ï¼š`find . -type d \( -name "test" -o -name "tests" -o -name "__tests__" -o -name "spec" \) -maxdepth 3 2>/dev/null`

**Pythonï¼š**
- æ£€æŸ¥ pytestï¼š`find . -name "pytest.ini" -o -name "conftest.py" -o -name "setup.cfg" 2>/dev/null | head -5`
- æ£€æŸ¥ unittestï¼š`find . -path "*/test*.py" -o -path "*/test_*.py" 2>/dev/null | head -5`
- æ£€æŸ¥è¦æ±‚ï¼š`grep -E "pytest|unittest|nose" requirements.txt 2>/dev/null`

**Rustï¼š**
- æ£€æŸ¥ Cargo æµ‹è¯•ï¼š`grep -E '\[dev-dependencies\]' Cargo.toml 2>/dev/null`
- æŸ¥æ‰¾æµ‹è¯•æ¨¡å—ï¼š`find . -name "*.rs" -exec grep -l "#\[cfg(test)\]" {} \; 2>/dev/null | head -5`

**Goï¼š**
- æ£€æŸ¥æµ‹è¯•æ–‡ä»¶ï¼š`find . -name "*_test.go" 2>/dev/null | head -5`
- æ£€æŸ¥ go.mod æ˜¯å¦å­˜åœ¨ï¼š`test -f go.mod && echo "Go æ¨¡å—å·²æ‰¾åˆ°"`

**å…¶ä»–è¯­è¨€ï¼š**
- Rubyï¼šæ£€æŸ¥ RSpecï¼š`find . -name ".rspec" -o -name "spec_helper.rb" 2>/dev/null`
- Javaï¼šæ£€æŸ¥ JUnitï¼š`find . -name "pom.xml" -exec grep -l "junit" {} \; 2>/dev/null`

### 2. æµ‹è¯•ç¯å¢ƒéªŒè¯

å¦‚æœæœªæ£€æµ‹åˆ°æµ‹è¯•æ¡†æ¶ï¼š
- å‘Šè¯‰ç”¨æˆ·ï¼š"âš ï¸ æœªæ£€æµ‹åˆ°æµ‹è¯•æ¡†æ¶ã€‚è¯·æŒ‡å®šæ‚¨çš„æµ‹è¯•è®¾ç½®ã€‚"
- è¯¢é—®ï¼š"æˆ‘åº”è¯¥ä½¿ç”¨ä»€ä¹ˆæµ‹è¯•å‘½ä»¤ï¼Ÿï¼ˆä¾‹å¦‚ï¼Œnpm testã€pytestã€cargo testï¼‰"
- å­˜å‚¨å“åº”ä»¥ä¾›å°†æ¥ä½¿ç”¨

### 3. ä¾èµ–é¡¹æ£€æŸ¥

**å¯¹äºæ£€æµ‹åˆ°çš„æ¡†æ¶ï¼š**
- Node.jsï¼šè¿è¡Œ `npm list --depth=0 2>/dev/null | grep -E "jest|mocha|chai|jasmine"`
- Pythonï¼šè¿è¡Œ `pip list 2>/dev/null | grep -E "pytest|unittest|nose"`
- éªŒè¯æµ‹è¯•ä¾èµ–é¡¹æ˜¯å¦å·²å®‰è£…

å¦‚æœç¼ºå°‘ä¾èµ–é¡¹ï¼š
- å‘Šè¯‰ç”¨æˆ·ï¼š"âŒ æµ‹è¯•ä¾èµ–é¡¹æœªå®‰è£…"
- å»ºè®®ï¼š"è¿è¡Œï¼šnpm installï¼ˆæˆ– pip install -r requirements.txtï¼‰"

## Instructions

### 1. Framework-Specific Configuration

Based on detected framework, create test configuration:

#### JavaScript/Node.js (Jest)
```yaml
framework: jest
test_command: npm test
test_directory: __tests__
config_file: jest.config.js
options:
  - --verbose
  - --no-coverage
  - --runInBand
environment:
  NODE_ENV: test
```

#### JavaScript/Node.js (Mocha)
```yaml
framework: mocha
test_command: npm test
test_directory: test
config_file: .mocharc.js
options:
  - --reporter spec
  - --recursive
  - --bail
environment:
  NODE_ENV: test
```

#### Python (Pytest)
```yaml
framework: pytest
test_command: pytest
test_directory: tests
config_file: pytest.ini
options:
  - -v
  - --tb=short
  - --strict-markers
environment:
  PYTHONPATH: .
```

#### Rust
```yaml
framework: cargo
test_command: cargo test
test_directory: tests
config_file: Cargo.toml
options:
  - --verbose
  - --nocapture
environment: {}
```

#### Go
```yaml
framework: go
test_command: go test
test_directory: .
config_file: go.mod
options:
  - -v
  - ./...
environment: {}
```

### 2. Test Discovery

Scan for test files:
- Count total test files found
- Identify test naming patterns used
- Note any test utilities or helpers
- Check for test fixtures or data

```bash
# Example for Node.js
find . -path "*/node_modules" -prune -o -name "*.test.js" -o -name "*.spec.js" | wc -l
```

### 3. Create Test Runner Configuration

Create `.claude/testing-config.md` with discovered information:

```markdown
---
framework: {detected_framework}
test_command: {detected_command}
created: [Use REAL datetime from: date -u +"%Y-%m-%dT%H:%M:%SZ"]
---

# Testing Configuration

## Framework
- Type: {framework_name}
- Version: {framework_version}
- Config File: {config_file_path}

## Test Structure
- Test Directory: {test_dir}
- Test Files: {count} files found
- Naming Pattern: {pattern}

## Commands
- Run All Tests: `{full_test_command}`
- Run Specific Test: `{specific_test_command}`
- Run with Debugging: `{debug_command}`

## Environment
- Required ENV vars: {list}
- Test Database: {if applicable}
- Test Servers: {if applicable}

## Test Runner Agent Configuration
- Use verbose output for debugging
- Run tests sequentially (no parallel)
- Capture full stack traces
- No mocking - use real implementations
- Wait for each test to complete
```

### 4. Configure Test-Runner Agent

Prepare agent context based on framework:

```markdown
# Test-Runner Agent Configuration

## Project Testing Setup
- Framework: {framework}
- Test Location: {directories}
- Total Tests: {count}
- Last Run: Never

## Execution Rules
1. Always use the test-runner agent from `.claude/agents/test-runner.md`
2. Run with maximum verbosity for debugging
3. No mock services - use real implementations
4. Execute tests sequentially - no parallel execution
5. Capture complete output including stack traces
6. If test fails, analyze test structure before assuming code issue
7. Report detailed failure analysis with context

## Test Command Templates
- Full Suite: `{full_command}`
- Single File: `{single_file_command}`
- Pattern Match: `{pattern_command}`
- Watch Mode: `{watch_command}` (if available)

## Common Issues to Check
- Environment variables properly set
- Test database/services running
- Dependencies installed
- Proper file permissions
- Clean test state between runs
```

### 5. Validation Steps

After configuration:
- Try running a simple test to validate setup
- Check if test command works: `{test_command} --version` or equivalent
- Verify test files are discoverable
- Ensure no permission issues

### 6. Output Summary

```
ğŸ§ª Testing Environment Primed

ğŸ” Detection Results:
  âœ… Framework: {framework_name} {version}
  âœ… Test Files: {count} files in {directories}
  âœ… Config: {config_file}
  âœ… Dependencies: All installed

ğŸ“‹ Test Structure:
  - Pattern: {test_file_pattern}
  - Directories: {test_directories}
  - Utilities: {test_helpers}

ğŸ¤– Agent Configuration:
  âœ… Test-runner agent configured
  âœ… Verbose output enabled
  âœ… Sequential execution set
  âœ… Real services (no mocks)

âš¡ Ready Commands:
  - Run all tests: /testing:run
  - Run specific: /testing:run {test_file}
  - Run pattern: /testing:run {pattern}

ğŸ’¡ Tips:
  - Always run tests with verbose output
  - Check test structure if tests fail
  - Use real services, not mocks
  - Let each test complete fully
```

### 7. Error Handling

**Common Issues:**

**No Framework Detected:**
- Message: "âš ï¸ No test framework found"
- Solution: "Please specify test command manually"
- Store user's response for future use

**Missing Dependencies:**
- Message: "âŒ Test framework not installed"
- Solution: "Install dependencies first: npm install / pip install -r requirements.txt"

**No Test Files:**
- Message: "âš ï¸ No test files found"
- Solution: "Create tests first or check test directory location"

**Permission Issues:**
- Message: "âŒ Cannot access test files"
- Solution: "Check file permissions"

### 8. Save Configuration

If successful, save configuration for future sessions:
- Store in `.claude/testing-config.md`
- Include all discovered settings
- Update on subsequent runs if changes detected

## Important Notes

- **Always detect** rather than assume test framework
- **Validate dependencies** before claiming ready
- **Configure for debugging** - verbose output is critical
- **No mocking** - use real services for accurate testing
- **Sequential execution** - avoid parallel test issues
- **Store configuration** for consistent future runs

$ARGUMENTS
